\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Formula Sheet}
\author{}
\date{}

\begin{document}
\maketitle

\section{Definition 1.1}
Mean of a sample with $n$ values:
\[
\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i
\]

\section{Definition 1.2}
Variance of a sample: overall distance of values from the mean:
\[
s^2 = \frac{1}{n-1}\sum_{i=1}^n (y_i - \bar{y})^2
\]

\section{Definition 1.3}
Standard Deviation ($s$): square root of variance ($s^2$):
\[
s = \sqrt{s^2}
\]

\section{Theorem 2.5}
Multiplicative Law of Probability: The probability of the intersection of two events:
\begin{enumerate}
  \item If dependent, $P(A \cap B) = P(A)P(B|A) = P(B)P(A|B)$.
  \item If independent, $P(A \cap B) = P(A)P(B)$.
\end{enumerate}

Additive Law of Probability: The probability of the union of two events:
\[
P(A \cup B) = P(A) + P(B) - P(A \cap B)
\]
If $P(A \cap B) = 0$ (mutually exclusive events), then $P(A \cup B) = P(A) + P(B)$.


\section{Definition 2.6}
Probability of an event $A$ within a sample space $S$, such that $A \subseteq S$. The following are true:
\begin{enumerate}
  \item $P(A) \geq 0$
  \item $P(S) = 1$
  \item If $(A_1, A_2, \ldots, A_n)$ are mutually exclusive, then $P(A_1 \cup A_2 \cup \ldots \cup A_n) = \sum_{i=1}^n P(A_i)$.
\end{enumerate}

\section{Definition 2.7}
Permutation: ordered arrangement of $r$ distinct objects, with $n$ possible orders:
\[
P_n^r = \frac{n!}{(n-r)!}
\]

\section{Theorem 2.7}
If $A$ is an event, then $P(A) = 1 - P(\bar{A})$.


\section{Definition 2.8}
Combination: number of subsets of size $r$ that can be formed from $n$ objects:
\[
C_n^r = \frac{n!}{r!(n-r)!}
\]

\section{Theorem 2.8}
Total probability: assume Definition 2.11. Then, for any event $A$:
\[
P(A) = \sum_{i=1}^k P(A|B_i)P(B_i)
\]

\section{Definition 2.9}
Conditional probability: chance event $A$ has occurred, given event $B$ has occurred (where $P(B) > 0$):
\[
P(A|B) = \frac{P(A \cap B)}{P(B)}
\]

\section{Definition 2.10}
Independence: The following must be true for events $A$ and $B$ to be independent. Otherwise, they are dependent:
\begin{enumerate}
  \item $P(A|B) = P(A)$
  \item $P(B|A) = P(B)$
  \item $P(A \cap B) = P(A)P(B)$
\end{enumerate}


\section{Definition 2.11}
Partition: for any positive integer $k$, $\{B_1, B_2, \ldots, B_k\}$ is a partition of sample space $S$ if:
\begin{enumerate}
  \item $S = B_1 \cup B_2 \cup \ldots \cup B_k$
  \item $B_i \cap B_j = \emptyset$ for all $i \neq j$
\end{enumerate}


\section{Bayes' Theorem}
For events $A$ and $B$ in space $S$ when $P(A) > 0$ and $P(B) > 0$:
\[
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
\]

\section{Definition 3.3}
Probability distribution for each value $y$ of random variable $Y$, given $0 \leq P(y) \leq 1$:
\[
p(y) = P(Y = y)
\]

\section{Definition 3.4}
Expected value of discrete random variable $Y$:
\[
\mu = E(Y) = \sum_{y} y p(y)
\]

\section{Definition 3.5}
Variance of discrete random variable $Y$:
\[
\sigma^2 = V(Y) = E(Y - \mu)^2
\]
Standard deviation of discrete random variable $Y$:
\[
\sigma = \sqrt{E(Y - \mu)^2}
\]

\section{Binomial Distribution}
Probability Mass Function for binomial variable $y$ with $n$ trials, success probability $p$, and failure probability $q$:
\[
P(Y = y) = \binom{n}{y} p^y q^{n-y}
\]

\section{Theorem 3.7}
Expected value of binomial random variable $Y$:
\[
\mu = E(Y) = np
\]
Variance of $Y$:
\[
\sigma^2 = V(Y) = npq
\]
Standard deviation of $Y$:
\[
\sigma = \sqrt{npq}
\]

\section{Definition 3.8}
Geometric probability distribution mass function, with success probability $p$ and failure probability $q$:
\[
P(Y = y) = q^{y-1}p
\]

\section{Theorem 3.8}
Expected value of geometric random variable $Y$:
\[
\mu = E(Y) = \frac{1}{p}
\]
Variance of $Y$:
\[
\sigma^2 = V(Y) = \frac{1-p}{p^2}
\]
Standard deviation of $Y$:
\[
\sigma = \sqrt{\frac{1-p}{p^2}}
\]

\section{Definition 3.9}
Negative binomial probability distribution, given $y = r, r+1, r+2, \ldots$ and $0 \leq P(y) \leq 1$. $y$ represents the trial where the $r$th success occurs, with success probability $p$:
\[
P(Y = y) = \binom{y-1}{r-1} p^r q^{y-r}
\]

\section{Theorem 3.9}
Expected value of negative binomial random variable $Y$:
\[
\mu = E(Y) = \frac{r}{p}
\]
Variance of $Y$:
\[
\sigma^2 = V(Y) = \frac{r(1-p)}{p^2}
\]
Standard deviation of $Y$:
\[
\sigma = \sqrt{\frac{r(1-p)}{p^2}}
\]

\section{Definition 3.10}
Hypergeometric probability distribution, given $y = 0, 1, 2, \ldots, n, y \leq r, n-y \leq N-r$. $n$ items are selected from $N$, with $r$ objects of desired type:
\[
P(Y = y) = \frac{\binom{r}{y}\binom{N-r}{n-y}}{\binom{N}{n}}
\]

\section{Theorem 3.10}
Expected value of hypergeometric random variable $Y$:
\[
\mu = E(Y) = \frac{nr}{N}
\]
Variance of $Y$:
\[
\sigma^2 = V(Y) = n \frac{r}{N}\frac{N-r}{N}\frac{N-n}{N-1}
\]
Standard deviation of $Y$:
\[
\sigma = \sqrt{n \frac{r}{N}\frac{N-r}{N}\frac{N-n}{N-1}}
\]

\section{Definition 3.11}
Poisson probability distribution of random variable $Y$:
\[
P(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!}
\]

\section{Theorem 3.11}
Expected value of Poisson distribution:
\[
E(Y) = \mu = \lambda
\]
Variance of Poisson distribution:
\[
V(Y) = \sigma^2 = \lambda
\]

\section{Theorem 3.14}
Tchebysheff's Theorem for random variable $Y$, mean $\mu$, and variance $\sigma^2$. For any constant $k > 0$:
\[
P(|Y - \mu| < k\sigma) \geq 1 - \frac{1}{k^2}
\]
or
\[
P(|Y - \mu| \geq k\sigma) \leq \frac{1}{k^2}
\]

\section{Definition 4.1}
Distribution function of $Y$ (any random variable):
\[
F(y) = P(Y \leq y) \quad \text{for all } -\infty < y < \infty
\]

\section{Theorem 4.1}
Properties of a distribution function:
\begin{enumerate}
  \item $F(-\infty) \equiv \lim\limits_{y \to -\infty} F(y) = 0$
  \item $F(\infty) \equiv \lim\limits_{y \to \infty} F(y) = 1$
  \item $F(y)$ is a nondecreasing function of $y$.
\end{enumerate}

\section{Definition 4.2}
If $F(y)$ is continuous on $-\infty < y < \infty$, $Y$ is a continuous random variable.

\section{Theorem 4.2}
Properties of a density function $f(y)$:
\begin{enumerate}
  \item $f(y) \geq 0 \quad \text{for all } -\infty < y < \infty$
  \item $\int_{-\infty}^{\infty} f(y)\,dy = 1$
\end{enumerate}

\section{Definition 4.3}
Probability density function of $Y$:
\[
f(y) = \frac{dF(y)}{dy} = F'(y)
\]

\section{Theorem 4.3}
If $Y$ has a density function $f(y)$ and bounds $a < b$, the probability that $Y$ is on interval $[a, b]$:
\[
P(a \leq Y \leq b) = \int_a^b f(y)\,dy
\]

\section{Theorem 4.4}
Expected value of a function $g(Y)$ of a continuous random variable $Y$:
\[
E[g(Y)] = \int_{-\infty}^{\infty} g(y) f(y)\,dy
\]

\section{Definition 4.5}
Expected value of continuous random variable $Y$:
\[
E(Y) = \int_{-\infty}^{\infty} y f(y)\,dy
\]

\section{Theorem 4.5}
Let $c$ be a constant and $g(Y), g_1(Y), \ldots, g_k(Y)$ be functions of $Y$:
\begin{enumerate}
  \item $E(c) = c$
  \item $E(cg(Y)) = cE[g(Y)]$
  \item $E(g_1(Y) + g_2(Y) + \cdots + g_k(Y)) = E[g_1(Y)] + E[g_2(Y)] + \cdots + E[g_k(Y)]$
\end{enumerate}

\section{Definition 4.6}
$Y$ has a continuous uniform probability distribution if its density function is:
\[
f(y) = \begin{cases} \frac{1}{\theta_2 - \theta_1}, & \theta_1 \leq y \leq \theta_2 \\ 0, & \text{elsewhere} \end{cases}
\]

\section{Theorem 4.6}
If $\theta_1 < \theta_2$ and $Y$ is uniformly distributed on $(\theta_1, \theta_2)$, then:
\begin{enumerate}
  \item $E(Y) = \mu = \frac{\theta_1 + \theta_2}{2}$
  \item $V(Y) = \sigma^2 = \frac{(\theta_2 - \theta_1)^2}{12}$
\end{enumerate}


\section{Theorem 4.7}
Expected value of normally distributed random variable $Y$:
\[
E(Y) = \mu
\]
Variance of normally distributed random variable $Y$:
\[
V(Y) = \sigma^2
\]
\section{Definition 4.8}
$Y$ has a normal probability distribution if its density function is:
\[
f(y) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(y-\mu)^2/(2\sigma^2)} \quad \text{when } \sigma > 0, -\infty < \mu < \infty, \text{ and } -\infty < y < \infty
\]

\section{Theorem 4.8}
Expected value of gamma distributed random variable $Y$:
\[
E(Y) = \mu = \alpha\beta
\]
Variance of gamma distributed random variable $Y$:
\[
V(Y) = \sigma^2 = \alpha\beta^2
\]

\section{Definition 4.9}
$Y$ has a gamma distribution with parameters $\alpha > 0$ and $\beta > 0$ if the density function is:
\[
f(y) = \begin{cases} \frac{y^{\alpha-1} e^{-y/\beta}}{\beta^\alpha \Gamma(\alpha)}, & 0 \leq y < \infty \\ 0, & \text{elsewhere} \end{cases}
\]
where $\Gamma(\alpha) = \int_0^{\infty} t^{\alpha-1} e^{-t}\,dt$.

\section{Theorem 4.10}
Expected value of exponentially distributed random variable $Y$:
\[
E(Y) = \mu = \beta
\]
Variance of exponentially distributed random variable $Y$:
\[
V(Y) = \sigma^2 = \beta^2
\]

\section{Definition 4.11}
$Y$ has an exponential distribution with parameter $\beta > 0$ if the density function is:
\[
f(y) = \begin{cases} \frac{1}{\beta} e^{-y/\beta}, & 0 \leq y < \infty \\ 0, & \text{elsewhere} \end{cases}
\]


\section{Definition 5.1}
Joint probability function of $Y_1$ and $Y_2$:
\[
p(y_1, y_2) = P(Y_1 = y_1, Y_2 = y_2) \quad \text{when } -\infty < y_1, y_2 < \infty
\]

\section{Theorem 5.1}
If $Y_1$ and $Y_2$ have a joint probability function, then:
\begin{enumerate}
  \item $p(y_1, y_2) \geq 0$
  \item $\sum_{y_1, y_2} p(y_1, y_2) = 1$
\end{enumerate}

\section{Definition 5.2}
Joint distribution function of $Y_1$ and $Y_2$:
\[
F(y_1, y_2) = P(Y_1 \leq y_1, Y_2 \leq y_2) \quad \text{when } -\infty < y_1, y_2 < \infty
\]


\section{Theorem 5.2}
If $Y_1$ and $Y_2$ have a joint distribution function, then:
\begin{enumerate}
  \item $F(-\infty, -\infty) = F(-\infty, y_2) = F(y_1, -\infty) = 0$
  \item $F(\infty, \infty) = 1$
  \item $f(y_1, y_2) \geq 0$
  \item $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(y_1, y_2) \, dy_1 \, dy_2 = 1$
\end{enumerate}


\section{Definition 5.3}
Jointly continuous random variables $Y_1$ and $Y_2$:
\[
F(y_1, y_2) = \int_{-\infty}^{y_1} \int_{-\infty}^{y_2} f(t_1, t_2) \, dt_2 \, dt_1
\]

\section{Definition 5.4}
Marginal probability function of $Y_1$ and $Y_2$:
\[
p_1(y_1) = \sum_{\text{all } y_2} p(y_1, y_2) \quad \text{and} \quad p_2(y_2) = \sum_{\text{all } y_1} p(y_1, y_2)
\]
Marginal density function of $Y_1$ and $Y_2$:
\[
f_1(y_1) = \int_{-\infty}^{\infty} f(y_1, y_2) \, dy_2 \quad \text{and} \quad f_2(y_2) = \int_{-\infty}^{\infty} f(y_1, y_2) \, dy_1
\]

\section{Theorem 5.4}
Marginal probability variables $Y_1$ and $Y_2$ are independent if and only if:
\[
p(y_1, y_2) = p_1(y_1)p_2(y_2) \quad \text{for every } (y_1, y_2)
\]
Otherwise, $Y_1$ and $Y_2$ are dependent.

Marginal density variables $Y_1$ and $Y_2$ are independent if and only if:
\[
f(y_1, y_2) = f_1(y_1)f_2(y_2) \quad \text{for every } (y_1, y_2)
\]
Otherwise, $Y_1$ and $Y_2$ are dependent.


\section{Definition 5.5}
Conditional discrete probability function of $Y_1$ and $Y_2$:
\[
p(y_1|y_2) = \frac{p(y_1, y_2)}{p_2(y_2)} \quad \text{when } p_2(y_2) > 0
\]

\section{Definition 5.6}
Conditional distribution function of $Y_1$ and $Y_2$:
\[
F(y_1|y_2) = P(Y_1 \leq y_1 | Y_2 = y_2)
\]

\section{Definition 5.7}
Conditional density of $Y_1$ given $Y_2 = y_2$:
\[
f(y_1|y_2) = \frac{f(y_1, y_2)}{f_2(y_2)}
\]
Conditional density of $Y_2$ given $Y_1 = y_1$:
\[
f(y_2|y_1) = \frac{f(y_1, y_2)}{f_1(y_1)}
\]

\section{Definition 5.8}
Joint distribution variables $Y_1$ and $Y_2$ are independent if and only if:
\[
F(y_1, y_2) = F_1(y_1)F_2(y_2) \quad \text{for every } (y_1, y_2)
\]
Otherwise, $Y_1$ and $Y_2$ are dependent.

\end{document}
